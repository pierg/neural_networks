[d1]
[d1.parameters]
k1_min = 0.01
k1_max = 0.05
k2_min = 0.01
k2_max = 0.05
k3_min = 0.01
k3_max = 0.05
E0_min = 0.1
E0_max = 0.3
S0_min = 0.1
S0_max = 0.3
spectra_min = -1
spectra_max = 1
amplitude_min = 0.2
amplitude_max = 0.8

[d1.dataset]
seed = 42
n_samples = 50000  # e.g., 50,000 samples
n_shards = 10  # The dataset is divided into 10 shards, each containing 5,000 samples
batch_size = 64  # Adjust based on your specific memory constraints
shuffle_buffer_size = 1000  # Sufficient for shuffling data effectively in most cases
splits = [0.7, 0.15, 0.15]  # 70% training, 15% validation, 15% test

# [d1.dataset]
# seed = 42
# n_samples = 1000000  # e.g., 1 million samples
# n_shards = 200  # The dataset is divided into 200 shards
# batch_size = 256  # Typical batch size for training on modern hardware
# shuffle_buffer_size = 2048  # Large enough for sufficient randomness, adjust based on available memory
# splits = [0.7, 0.15, 0.15]  # 70% training, 15% validation, 15% test

# [d1.dataset]
# seed = 42
# n_samples = 5
# n_shards = 1
# batch_size = 1 
# shuffle_buffer_size = 5
# splits = [0.6, 0.2, 0.2]

[d2]
[d2.parameters]
k1_min = 0.01
k1_max = 0.05
k2_min = 0.01
k2_max = 0.05
k3_min = 0.01
k3_max = 0.05
E0_min = 0.1
E0_max = 0.3
S0_min = 0.1
S0_max = 0.3
spectra_min = -1
spectra_max = 1
amplitude_min = 0.2
amplitude_max = 0.8
[d2.dataset]
seed = 55
n_samples = 2
batch_size = 32
shuffle_buffer_size = 512
splits = [0.7, 0.15, 0.15]

